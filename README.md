This project focuses on predicting healthcare outcomes using Machine Learning models combined with Explainable AI (XAI) techniques to ensure transparency, trust, and interpretability in medical decision-making. Traditional AI models often behave like “black boxes,” making it difficult to understand why a prediction was made. To address this challenge, this project integrates explainability methods that highlight the most influential clinical features such as symptoms, medical history, and laboratory results.

The workflow includes data preprocessing, feature engineering, model training, evaluation, and interpretation. Multiple machine learning algorithms were implemented and compared to achieve accurate predictions, while XAI techniques like SHAP and LIME were used to provide human-understandable explanations of model outputs. These explanations help healthcare professionals validate predictions, reduce bias, and support reliable clinical decisions.
